# -*- coding: utf-8 -*-
"""mini_project_draft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sBPsLCuUCIzL-n8ksSWWqkxQ8gXCQvaj
"""

import datetime
import pandas as pd
import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Deep Learning Imports
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Module 1: Data Fetching & Cleanup
# ===============================
def fetch_data(symbol, start, end):
    ticker = yf.download(symbol, start, end)
    df = pd.DataFrame(ticker)
    # Flatten multi-index columns if present
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)
    return df

# Module 2: Technical Indicators (without Bollinger Bands and Stochastic Oscillator)
# ===============================
def calculate_ema(data, period=60):
    """Return a DataFrame with column EMA_{period}."""
    multiplier = 2 / (period + 1)
    ema = pd.Series(index=data.index, dtype=np.float64)
    ema.iloc[0] = data['Close'].iloc[0]
    for i in range(1, len(data)):
        ema.iloc[i] = (data['Close'].iloc[i] * multiplier) + (ema.iloc[i-1] * (1 - multiplier))
    return pd.DataFrame({f'EMA_{period}': ema}, index=data.index)

def calculate_parabolic_sar(data, af=0.02, af_max=0.2):
    """Return a DataFrame with column 'PSAR' computed from data."""
    high = data['High'].values
    low = data['Low'].values
    n = len(data)
    psar = np.zeros(n, dtype=np.float64)
    trend = np.ones(n)  # 1: uptrend, -1: downtrend
    ep = np.zeros(n, dtype=np.float64)
    af_current = af
    psar[0] = low[0]
    ep[0] = high[0]
    for i in range(1, n):
        psar[i] = psar[i-1] + af_current * (ep[i-1] - psar[i-1])
        if trend[i-1] == 1:
            if i >= 2:
                psar[i] = min(psar[i], low[i-1], low[i-2])
            else:
                psar[i] = min(psar[i], low[i-1])
        else:
            if i >= 2:
                psar[i] = max(psar[i], high[i-1], high[i-2])
            else:
                psar[i] = max(psar[i], high[i-1])
        if trend[i-1] == 1:
            if low[i] < psar[i]:
                trend[i] = -1
                psar[i] = ep[i-1]
                ep[i] = low[i]
                af_current = af
            else:
                trend[i] = 1
                if high[i] > ep[i-1]:
                    ep[i] = high[i]
                    af_current = min(af_current + af, af_max)
                else:
                    ep[i] = ep[i-1]
        else:
            if high[i] > psar[i]:
                trend[i] = 1
                psar[i] = ep[i-1]
                ep[i] = high[i]
                af_current = af
            else:
                trend[i] = -1
                if low[i] < ep[i-1]:
                    ep[i] = low[i]
                    af_current = min(af_current + af, af_max)
                else:
                    ep[i] = ep[i-1]
    return pd.DataFrame({'PSAR': psar}, index=data.index)

# Module 3: Construct Signals (Combine Indicators & Features)
# ===============================
def construct_signals(data, ema_period=60, psar_af=0.02, psar_af_max=0.2):
    data = data.copy()
    ema_df = calculate_ema(data, ema_period)
    psar_df = calculate_parabolic_sar(data, psar_af, psar_af_max)

    data = data.join(ema_df)
    data = data.join(psar_df)

    # Compute additional features:
    data['Trend'] = (data['Open'] - data[f'EMA_{ema_period}']) * 100
    data['Return'] = data['Close'].pct_change() * 100
    data['Direction'] = np.where(data['Close'] > data['Open'], 1, -1)

    features = ['Trend', 'Volume', f'EMA_{ema_period}', 'PSAR', 'Return']
    data = data.dropna()
    return data[features + ['Direction']]

# Module 4: Create Sequences for LSTM from Multiple Features
# ===============================
def create_sequences_multifeature(data, feature_cols, sequence_length=10):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        seq = data[feature_cols].iloc[i:i+sequence_length].values
        label = data['Direction'].iloc[i+sequence_length]
        X.append(seq)
        y.append(label)
    return np.array(X), np.array(y)

# Module 5: LSTM Model Training with Early Stopping
# ===============================
def build_lstm_model(input_shape, units1=50, units2=30, dropout_rate=0.2, learning_rate=0.001):
    model = Sequential()
    model.add(LSTM(units1, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(dropout_rate))
    model.add(LSTM(units2))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def train_lstm_model(X_train, y_train, X_val, y_val, epochs=50, batch_size=16):
    input_shape = (X_train.shape[1], X_train.shape[2])
    model = build_lstm_model(input_shape)
    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                        epochs=epochs, batch_size=batch_size, callbacks=[early_stop], verbose=1)
    return model, history
#The training stopped before reaching 50 epochs because an EarlyStopping callback was used.
#This callback monitors the validation loss and stops training if there is no improvement for a specified number of epochs (in this case, 5 epochs).
#This is done to prevent overfitting and save computation time.

# Module 6: Data Visualization
# ===============================
def plot_volume(data):
    plt.figure(figsize=(10,4))
    if 'Volume' in data.columns:
        plt.bar(data.index, data['Volume'], color='grey')
        plt.title('Volume Over Time')
        plt.xlabel('Date')
        plt.ylabel('Volume')
        plt.tight_layout()
        plt.show()
    else:
        print("Volume column not found.")

def plot_trend(data):
    plt.figure(figsize=(10,4))
    if 'Trend' in data.columns:
        plt.plot(data.index, data['Trend'], color='purple')
        plt.title('Trend Over Time')
        plt.xlabel('Date')
        plt.ylabel('Trend')
        plt.tight_layout()
        plt.show()
    else:
        print("Trend column not found.")

def plot_daily_return(data):
    plt.figure(figsize=(10,4))
    if 'Return' in data.columns:
        plt.plot(data.index, data['Return'], color='green')
        plt.title('Daily Return (%) Over Time')
        plt.xlabel('Date')
        plt.ylabel('Daily Return (%)')
        plt.tight_layout()
        plt.show()
    else:
        print("Return column not found.")

def plot_correlation_heatmap(data):
    plt.figure(figsize=(10,8))
    corr = data.corr()
    sns.heatmap(corr, annot=True, cmap='coolwarm')
    plt.title('Feature Correlations')
    plt.tight_layout()
    plt.show()

def plot_close_with_ema_psar(data, ema_period=60):
    plt.figure(figsize=(14,8))
    if 'Close' in data.columns and f'EMA_{ema_period}' in data.columns and 'PSAR' in data.columns:
        plt.plot(data.index, data['Close'], label='Close Price', color='black', linewidth=1.5)
        plt.plot(data.index, data[f'EMA_{ema_period}'], label=f'EMA_{ema_period}', color='blue', linestyle='--')
        plt.plot(data.index, data['PSAR'], label='PSAR', color='red', marker='o', linestyle='-', markersize=3)
        plt.title('Close Price with EMA and PSAR')
        plt.xlabel('Date')
        plt.ylabel('Price')
        plt.legend()
        plt.tight_layout()
        plt.show()
    else:
        print("Required columns for close, EMA, or PSAR not found.")

import matplotlib.pyplot as plt

def plot_training_history(history):
    """Plots the training and validation loss & accuracy over epochs."""
    plt.figure(figsize=(12, 5))

    # Plot Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Model Accuracy')
    plt.legend()

    # Plot Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Model Loss')
    plt.legend()

    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred):
    """Plots a confusion matrix using seaborn."""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.show()

# Main Execution
# ===============================
if __name__ == '__main__':
    # Set date range and symbol
    start_date = datetime.datetime(2024, 1, 1)
    end_date = datetime.datetime(2025, 1, 1)
    symbol = 'JPM'

    # Fetch and process data
    raw_data = fetch_data(symbol, start_date, end_date)
    signals_data = construct_signals(raw_data, ema_period=60, psar_af=0.02, psar_af_max=0.2)
    signals_data_clean = signals_data.dropna()
    if signals_data_clean.empty:
        signals_data_clean = raw_data.fillna(method='bfill').fillna(method='ffill')
        signals_data_clean = construct_signals(signals_data_clean)

    print(signals_data_clean.head())

    # Plot individual visualizations:
    plot_volume(signals_data_clean)
    plot_trend(signals_data_clean)
    plot_daily_return(signals_data_clean)
    plot_correlation_heatmap(signals_data_clean)

    # Finally, plot Close Price with EMA and PSAR
    # We need EMA_60 column for this plot. Let's ensure it's present.
    if f'EMA_60' not in signals_data_clean.columns:
        signals_data_clean = construct_signals(raw_data, ema_period=60, psar_af=0.02, psar_af_max=0.2)
    plot_close_with_ema_psar(signals_data_clean, ema_period=60)

    # Prepare data for LSTM model using multiple features:
    # Features: Trend, EMA_60, PSAR, Volume, Return
    feature_cols = ['Trend', 'EMA_60', 'PSAR', 'Volume', 'Return']
    # Convert Direction from {1, -1} to {1, 0} for binary classification
    signals_data_clean['Direction'] = np.where(signals_data_clean['Direction'] == 1, 1, 0)

    # Scale features
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(signals_data_clean[feature_cols])
    scaled_df = pd.DataFrame(scaled_features, index=signals_data_clean.index, columns=feature_cols)
    # Add target column back
    scaled_df['Direction'] = signals_data_clean['Direction']

    # Create sequences from multiple features
    sequence_length = 10
    X, y = create_sequences_multifeature(scaled_df, feature_cols, sequence_length=sequence_length)

    # Split into training and validation sets (80/20 split)
    split_idx = int(len(X) * 0.8)
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]

    # Build and train LSTM model with early stopping
    model, history = train_lstm_model(X_train, y_train, X_val, y_val, epochs=50, batch_size=16)

    # Plot training history and confusion matrix
    plot_training_history(history)

    # Evaluate on validation set
    y_pred_prob = model.predict(X_val)
    y_pred = (y_pred_prob > 0.5).astype(int).flatten()
    val_accuracy = accuracy_score(y_val, y_pred)
    conf_matrix = confusion_matrix(y_val, y_pred)

    print(f"Validation Accuracy: {val_accuracy:.2f}")
    print("Confusion Matrix:")
    print(conf_matrix)
    plot_confusion_matrix(y_val, y_pred)

